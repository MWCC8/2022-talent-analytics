---
title: "S6 Prediction"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

##### setup -----
rm(list = ls())
gc()

library(tidyverse)
library(skimr)
library(lubridate)
library(arrow)
library(modelsummary)
set.seed(10101)

##### paths -----
data_path = "~/Dropbox/McGill/teaching/2021-2022/2022_winter/ORGB671/data/"

###### S4 regressions -----
###### > some prep -----
# Load examiner GS data
examiner_gs <- read_csv(paste0(data_path,"examiner_gs.csv"))

##### >> calculate duration in grade -----
# Use latest observed date to replace NAs
max_end_date <- examiner_gs %>% 
  summarise(max(mdy(end_date,"m/d/y"), na.rm = TRUE)) %>% 
  pull()
# get number of days in a GS grade
examiner_gs <- examiner_gs %>% 
  mutate(
    start_date_old = start_date, # for manual verification
    end_date_old = end_date, # for manual verification
    start_date = mdy(start_date_old),
    end_date = if_else(is.na(end_date),max_end_date,mdy(end_date)),
    days_in_grade = interval(start_date, end_date) %/% days(1)
  ) %>% 
  select(-start_date_old,-end_date_old)


##### >> add examiner gender -----
# Using a modified example from https://cran.r-project.org/web/packages/gender/vignettes/predicting-gender.html
library(gender)
#install_genderdata_package() # only run the first time
examiner_gender <- examiner_gs %>% 
  mutate(
    name = examiner_name,
    ff_name = str_extract(examiner_name,"(?<=,\\s)\\w+"), # extract first first name
  ) %>% 
  distinct(ff_name) %>% 
  do(results = gender(.$ff_name, method = "ssa")) %>% 
  unnest(cols = c(results), keep_empty = TRUE) %>% 
  select(
    ff_name = name,
    gender,
    proportion_female
  )
# joining gender back to the dataset
examiner_gs <- examiner_gs %>% 
  mutate(
    ff_name = str_extract(examiner_name,"(?<=,\\s)\\w+"), # extract first first name
  ) %>% 
  left_join(examiner_gender, by = "ff_name")
# cleaning up
rm(examiner_gender)
gc()

# load AU data
examiner_au <- read_csv(paste0(data_path,"examiner_aus.csv"))
```

## Predict mobility across art units as a function of tenure and TC

We will be using data from `examiner_aus.csv` file. The file contains the data on for art units (AUs) where the examiner worked in a given month. Let's count the number of moves as the number of distinct AUs associated with an examiner, minus one.

```{r}
# count examiner moves
examiner_moves <- examiner_au %>% 
  arrange(old_pid,year,month) %>% 
  distinct(old_pid,examiner_art_unit) %>% # keep unique examiner-AU combinations
  group_by(old_pid) %>% 
  mutate(
    au = examiner_art_unit,
    tc = floor(au/100)*100,
    moves = (n()-1)
  ) %>% 
  ungroup()

datasummary_skim(examiner_moves, histogram=FALSE)
```

There are only [9 TCs at the organization](https://www.uspto.gov/patents/contact-patents/patent-technology-centers-management), not 97. Some AU values must be wrong.
Each legitimate TC likely has 1000 or more examiners, so we'll filter on that.

```{r}
# adjust for the wrong AU values
examiner_moves <- examiner_au %>% 
  arrange(old_pid,year,month) %>% 
  distinct(old_pid,examiner_art_unit) %>% 
  mutate(tc = floor(examiner_art_unit/100)*100) %>% 
  add_count(tc) %>% 
  mutate(
    tc = if_else(n<1000,NA_real_,tc), # TCs with fewer than 1K examiners are not real
  ) %>% 
  filter(!is.na(tc)) %>% # drop them
  select(-n) %>% 
  group_by(old_pid) %>% 
  mutate(
    first_au = if_else(row_number()==1,examiner_art_unit,0),
    moves = (n()-1),
    has_moved = if_else(moves>0,1,0)
  ) %>% 
  ungroup() %>% 
  filter(first_au!=0) %>% 
  select(
    old_pid,
    first_au,
    first_tc = tc,
    moves,
    has_moved
  )
datasummary_skim(examiner_moves, histogram=FALSE)
```

Now, we will add gender and tenure variables.

```{r}
# add examiner gender and tenure
examiner_gender_tenure <- examiner_gs %>% 
  filter(!is.na(gender)) %>% 
  mutate(
    woman = if_else(gender=="female",1,0)
  ) %>%
  group_by(old_pid) %>% 
  summarise(
    woman = max(woman),
    tenure = sum(days_in_grade)/365
  )

datasummary_skim(examiner_gender_tenure, histogram=FALSE)
```

Some tenure values are clearly out of whack. Negative 52 years! Setting them to missing.

```{r}
# remove negative tenure values
examiner_gender_tenure <- examiner_gender_tenure %>% 
  filter(tenure>0)

# add back to the examiner_moves
examiner_moves <- examiner_moves %>% 
  left_join(examiner_gender_tenure)

datasummary_skim(examiner_gender_tenure, histogram=FALSE)
```
```{r, include=FALSE}
# clean up
rm(examiner_gender_tenure)
gc()

# load model packages
library(modelr)
library(infer)
```

## Model selection

First, let's see how tenure predicts mobility across AUs.

```{r}
ols1 <-  lm(has_moved ~ tenure, data = examiner_moves)
modelsummary(ols1)
```
Looks like the relationship is negative, though not very strong. Now, let's look at what produces $R^2$. First, we estimate predicted values based on $\hat{\beta}$ that the model has estimated. 

```{r}
examiner_moves <- examiner_moves %>% 
  add_predictions(ols1, var = "P_move")
examiner_moves
```

The field `P_move` contains estimated probability that an examiner has moved art units, based on the simple model above (i.e., just on the value of tenure in years). If we estimate the correlation between these predicted values (i.e., $\hat{Y}$) and true observed values in column `has_moved` (i.e., $Y$), we get a number.

```{r, warning=FALSE}
corr_ols1 <- examiner_moves %>% 
  observe(has_moved ~ P_move, stat = "correlation") %>% 
  pull(stat)
corr_ols1
```
If we square that number, we get the $R^2$ value from the regression output above! It's as simple as that. 

```{r}
corr_ols1^2
```

This is a model fit statistic. But today we are interested in prediction.

## Prediction

To assess how good our model is for predicting outcomes, we need to do a few things. First, we need to make sure we use a random sample of data to estimate the model and some other *different* part of the data to test it. Otherwise we are risking overfitting the model, so that its prediction is good within the sample, but not so good with any other data.

```{r}
# estimation sample
est_sample <- examiner_moves %>% 
  select(
    old_pid,
    first_au,
    first_tc,
    has_moved,
    woman,
    tenure
  ) %>% 
  slice_sample(n = 9652) # randomly select about 85% of the data

# holdout data for validation
test_sample <- examiner_moves %>% 
  anti_join(est_sample) %>% 
  select(
    old_pid,
    first_au,
    first_tc,
    has_moved,
    woman,
    tenure
  ) 
```

Let's estimate the same model on our training data.

```{r}
pr_ols1 <- lm(has_moved ~ tenure, data = est_sample)

modelsummary(pr_ols1)

test_sample <- test_sample %>% 
  add_predictions(pr_ols1, var = "P_move")
```

The model estimates look similar to the ones we had based on the full sample. This is expected, since we are using a random subset of that sample.

### Thresholds

The question is what to do with these predicted values? As you can see, we have predicted probability of move `P_move`, but an examiner either moves or doesn't. How do we deal with that? One way is to set a threshold for this binary prediction. If Pr(move) is higher than that threshold, we'll count this prediction as predicting a move. If it's lower, we'll count such prediction as no move.

But where so set this threshold? The distribution of `P_move` is continuous and it's not obvious where to cut it:

```{r hist, warning=FALSE}
g <- ggplot(data = test_sample, mapping = aes(x = P_move))
g + geom_histogram(orientation = "")
```

We'll use **sensitivity** (True Positive Rate, TPR) and its inverse (False Positive Rate, FPR) to see how various thresholds are affecting the quality of our prediction, given the model.

Let's pick a threshold that's close to the center of the distribution, say 0.7, and generate predicted binary outcome for the move, given this threshold `pr_moved_tr1`.

```{r}
tr1 <- 0.7

test_sample <- test_sample %>% 
  mutate(pr_moved_tr1 = if_else(P_move>tr1,1,0))
test_sample
```

Let's calculate TPR and FPR based on true/false positives and true/false negatives. 

```{r}
gof_tr1 <- test_sample %>% 
  filter(!is.na(P_move)) %>% 
  mutate(
    TP = if_else(pr_moved_tr1==1 & has_moved==1,1,0),
    FP = if_else(pr_moved_tr1==1 & has_moved==0,1,0),
    TN = if_else(pr_moved_tr1==0 & has_moved==0,1,0),
    FN = if_else(pr_moved_tr1==0 & has_moved==1,1,0)
  )  %>% 
  summarise(
    a = sum(TP),
    b = sum(FN),
    c = sum(FP),
    d = sum(TN)
  ) %>% 
  mutate(
    TPR = a/(a+b),
    FPR = c/(c+d)
  )

gof_tr1
```
So, with this threshold, we'll predict a move correctly when it happened 28 percent of the time. But note that we also predict the move when it didn't actually happen 25.5 percent of the time.

Can we do better with a different threshold? Let's try a smaller one, say 0.6

```{r}
tr2 <- 0.6

test_sample <- test_sample %>% 
  mutate(pr_moved_tr2 = if_else(P_move>tr2,1,0))

gof_tr2 <- test_sample %>% 
  filter(!is.na(P_move)) %>% 
  mutate(
    TP = if_else(pr_moved_tr2==1 & has_moved==1,1,0),
    FP = if_else(pr_moved_tr2==1 & has_moved==0,1,0),
    TN = if_else(pr_moved_tr2==0 & has_moved==0,1,0),
    FN = if_else(pr_moved_tr2==0 & has_moved==1,1,0)
  )  %>% 
  summarise(
    a = sum(TP),
    b = sum(FN),
    c = sum(FP),
    d = sum(TN)
  ) %>% 
  mutate(
    TPR = a/(a+b),
    FPR = c/(c+d)
  )

gof_tr2
```
Well, our true positive rate is definitely better. We predict a move correctly 93% of the time! But look at what happened with the false positives -- we predict a move when there was no move 89% of the time. That's a lot of false predictions!

It seems that by moving the threshold we are engaging in a tradeoff. One type of error is reduced, but that increases another type. Let's try one more threshold.

```{r}
tr3 <- 0.75

test_sample <- test_sample %>% 
  mutate(pr_moved_tr3 = if_else(P_move>tr3,1,0))

gof_tr3 <- test_sample %>% 
  filter(!is.na(P_move)) %>% 
  mutate(
    TP = if_else(pr_moved_tr3==1 & has_moved==1,1,0),
    FP = if_else(pr_moved_tr3==1 & has_moved==0,1,0),
    TN = if_else(pr_moved_tr3==0 & has_moved==0,1,0),
    FN = if_else(pr_moved_tr3==0 & has_moved==1,1,0)
  )  %>% 
  summarise(
    a = sum(TP),
    b = sum(FN),
    c = sum(FP),
    d = sum(TN)
  ) %>% 
  mutate(
    TPR = a/(a+b),
    FPR = c/(c+d)
  )

gof_tr3
```
Again, moving along the tradeoff, but in the other direction now. The false predictions are very low, but the true predictions are terrible.

## ROC plots

You can imagine that we can be selecting the threshold in small increments, which can produce a set of values. We can then plot that set of values on what's called ROC plot.

```{r}
start_end <- tibble(TPR = c(0,1), FPR = c(0,1))
## ROC
roc_data <- gof_tr1 %>% 
  add_row(gof_tr2) %>% 
  add_row(gof_tr3) %>% 
  select(TPR,FPR) %>% 
  add_row(start_end)

roc_data
```

```{r, echo=TRUE, warning=FALSE}
g <- ggplot(roc_data, aes(x = FPR, y= TPR))

g + geom_point() +
  geom_smooth() +
  geom_abline(intercept = 0) +
  xlim(0,1)+
  ylim(0,1)
```

[ROC curves](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) are useful because we can compare how different models -- or, in fact, any prediction methods -- perform on the prediction tasks. The closer the curve is to the top left corner, the better the prediction.


